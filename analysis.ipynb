{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import madmom\n",
    "import mir_eval\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from modules.labels import get_label_vector\n",
    "from modules.madmom_cnn_prep import cnn_preprocessor\n",
    "from datasets import Dataset\n",
    "from modules.analysis_funcs import get_idx_to_fold, get_segmented_data, get_test_peaks, aubio_peakpicker_do, aubio_postprocessing\n",
    "from analyze_detection import evaluate\n",
    "from modules.energy_based import legato_mg, onsets_threshold_gate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "FPS = 100\n",
    "CONTEXT = 7\n",
    "\n",
    "# Load Madmom normalization\n",
    "def cnn_normalize(frames):\n",
    "    inv_std = np.load(\"models/bock2013pret_inv_std.npy\")\n",
    "    mean = np.load(\"models/bock2013pret_mean.npy\")\n",
    "    frames_normalized = (frames - np.reshape(mean, (1,80,3)))*np.reshape(inv_std, (1,80,3))\n",
    "    return frames_normalized\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = madmom.audio.signal.FramedSignalProcessor(frame_size=2048, hop_size=441)\n",
    "stft = madmom.audio.stft.STFTProcessor()\n",
    "spect = madmom.audio.spectrogram.SpectrogramProcessor()\n",
    "proc = madmom.processors.SequentialProcessor([frame, stft, spect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/audio/signal.py:457: UserWarning: Deprecated as of version 0.16. Please use madmom.io.audio.load_wave_file instead. Will be removed in version 0.18.\n",
      "  warnings.warn('Deprecated as of version 0.16. Please use madmom.io.audio.'\n",
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    }
   ],
   "source": [
    "ds0 = Dataset(\"initslurtest\")\n",
    "ds1 = Dataset(\"slurtest_add_1\")\n",
    "\n",
    "audio_fnames = ds0.get_audio_paths() + ds1.get_audio_paths()\n",
    "label_fnames = ds0.get_annotation_paths() + ds1.get_annotation_paths()\n",
    "\n",
    "audios = [madmom.audio.signal.load_wave_file(filename)[0] for filename in audio_fnames]\n",
    "sample_rates = [madmom.audio.signal.load_wave_file(filename)[1] for filename in audio_fnames]\n",
    "onset_schedules = [np.loadtxt(label_fname, usecols=0) for label_fname in label_fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3556419933217189\n"
     ]
    }
   ],
   "source": [
    "# Dataset info\n",
    "audio_lengths_sec = [len(audio)/sr for audio,sr in zip(audios, sample_rates)]\n",
    "iois = [np.ediff1d(onsets) for onsets in onset_schedules]\n",
    "ioi_spreads = [np.std(np.ediff1d(onsets)) for onsets in onset_schedules]\n",
    "onset_number = [len(onsets) for onsets in onset_schedules]\n",
    "mean_ioi = np.sum([np.sum(ioi) for ioi in iois])/np.sum(onset_number)\n",
    "print(mean_ioi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 119\n",
    "n_splits =  5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=True)\n",
    "kf_gen = list(kf.split(np.arange(len(audio_fnames))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"results/cnn-training-220409/\"\n",
    "folds_path = base_path + \"folds.pkl\"\n",
    "\n",
    "model_name = \"added-sample-gen-nostandard\"\n",
    "\n",
    "folds = kf_gen\n",
    "#with open(folds_path, \"rb\") as f:\n",
    "    #folds = pickle.load(f)\n",
    "\n",
    "itf = get_idx_to_fold(folds)\n",
    "\n",
    "neural = False\n",
    "TOL = 0.025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_list = []\n",
    "FN_list = []\n",
    "FP_list = []\n",
    "for r in range(len(itf.keys())):\n",
    "    fold = itf[r]\n",
    "    rec_name = os.path.basename(audio_fnames[r])\n",
    "    x = get_segmented_data(audio_fnames[r])\n",
    "    if neural:\n",
    "        model = tf.keras.models.load_model(base_path + \"fold_\" + str(fold) + \"_\" + model_name + \"_model\")\n",
    "        out = model.predict(x)\n",
    "        peaks = get_test_peaks(out, 1./FPS)\n",
    "    else:\n",
    "        leg_on, leg_val = legato_mg(audio_fnames[r], rel_delta=0.2)\n",
    "        peaks = onsets_threshold_gate(leg_on, leg_val, 1.0)\n",
    "    \n",
    "    [CD,FN,FP,doubles,merged] = evaluate(onset_schedules[r], peaks, tol_sec=TOL)\n",
    "    CD_list.append(CD)\n",
    "    FN_list.append(FN)\n",
    "    FP_list.append(FP)\n",
    "\n",
    "    scores = mir_eval.onset.evaluate(onset_schedules[r], peaks, window=TOL)\n",
    "    print(rec_name + \"\\t\" + \"F-score: {:.2f}\".format(100*scores[\"F-measure\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.26315789473685\n",
      "19.88950276243094\n",
      "63.128491620111724\n",
      "56.92307692307692\n",
      "43.39622641509434\n",
      "6.862745098039215\n",
      "76.30057803468206\n",
      "66.19718309859154\n",
      "0.5162012023084546\n"
     ]
    }
   ],
   "source": [
    "# Only first fold\n",
    "#model = tf.keras.models.load_model(\"results/cnn-training-220409/fold_0_added-seq-gen-nostandard_model\")\n",
    "av_scores = []\n",
    "for r in folds[0][1]:\n",
    "    rec_name = os.path.basename(audio_fnames[r])\n",
    "    sig = madmom.audio.Signal(audio_fnames[r])\n",
    "    spect = madmom.audio.spectrogram.Spectrogram(audio_fnames[r])\n",
    "    hfc = madmom.features.onsets.high_frequency_content(spect)\n",
    "    onehot, out = aubio_peakpicker_do(hfc, threshold=0.15)\n",
    "    peaks = aubio_postprocessing(onehot, sig, db_thres=-90, min_ioi_frames=6)/FPS\n",
    "    \n",
    "    #x = get_segmented_data(audio_fnames[r])\n",
    "    #out = model.predict(x)\n",
    "    #peaks = get_test_peaks(out, 1./FPS)\n",
    "    scores = mir_eval.onset.evaluate(onset_schedules[r], peaks, window=TOL)\n",
    "    print(scores[\"F-measure\"]*100)\n",
    "    av_scores.append(scores[\"F-measure\"])\n",
    "print(np.mean(av_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "slurtest04.wav\tF-score: 80.26\n",
      "slurtest18.wav\tF-score: 19.89\n",
      "stormhatten_IR2.wav\tF-score: 63.13\n",
      "slurtest01_IR2.wav\tF-score: 56.92\n",
      "6xtpsg_220319.wav\tF-score: 43.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6xtpsg_220306.wav\tF-score: 6.86\n",
      "slurtest04_FK1.wav\tF-score: 76.30\n",
      "slurtest03_IR1.wav\tF-score: 66.20\n",
      "Fold 0    average F-score 51.62\n",
      "\n",
      "1\n",
      "slurtest05.wav\tF-score: 68.27\n",
      "slurtest04_IR2.wav\tF-score: 78.48\n",
      "melodyvib_220319.wav\tF-score: 24.74\n",
      "slurtest09_IR2.wav\tF-score: 55.95\n",
      "janissa_IR2.wav\tF-score: 45.02\n",
      "slurtest01_FK1.wav\tF-score: 59.31\n",
      "slurtest08_FK1.wav\tF-score: 71.93\n",
      "Fold 1    average F-score 57.67\n",
      "\n",
      "2\n",
      "slurtest03.wav\tF-score: 74.17\n",
      "slurtest11.wav\tF-score: 29.76\n",
      "slurtest15.wav\tF-score: 57.94\n",
      "slurtest02_IR1.wav\tF-score: 50.75\n",
      "slurtest03_FK1.wav\tF-score: 62.32\n",
      "slurtest01_IR1.wav\tF-score: 60.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/madmom/io/audio.py:493: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  file_sample_rate, signal = wavfile.read(filename, mmap=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63an_start_220306.wav\tF-score: 10.34\n",
      "Fold 2    average F-score 49.39\n",
      "\n",
      "3\n",
      "slurtest02.wav\tF-score: 71.52\n",
      "slurtest07.wav\tF-score: 70.88\n",
      "slurtest08.wav\tF-score: 72.99\n",
      "slurtest14.wav\tF-score: 56.94\n",
      "slurtest17.wav\tF-score: 39.65\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "frames must be a 2D array or iterable, got <class 'madmom.audio.signal.FramedSignal'> with shape (5663, 2048, 2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/simonfal/kth-master/exjobb/beyondPianoRoll/analysis.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonfal/kth-master/exjobb/beyondPianoRoll/analysis.ipynb#ch0000008?line=13'>14</a>\u001b[0m     peaks \u001b[39m=\u001b[39m get_test_peaks(out, \u001b[39m1.\u001b[39m\u001b[39m/\u001b[39mFPS)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonfal/kth-master/exjobb/beyondPianoRoll/analysis.ipynb#ch0000008?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/simonfal/kth-master/exjobb/beyondPianoRoll/analysis.ipynb#ch0000008?line=15'>16</a>\u001b[0m     spect \u001b[39m=\u001b[39m madmom\u001b[39m.\u001b[39;49maudio\u001b[39m.\u001b[39;49mspectrogram\u001b[39m.\u001b[39;49mSpectrogram(audio_fnames[r])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonfal/kth-master/exjobb/beyondPianoRoll/analysis.ipynb#ch0000008?line=16'>17</a>\u001b[0m     hfc \u001b[39m=\u001b[39m madmom\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39monsets\u001b[39m.\u001b[39mhigh_frequency_content(spect)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/simonfal/kth-master/exjobb/beyondPianoRoll/analysis.ipynb#ch0000008?line=17'>18</a>\u001b[0m     onehot, out \u001b[39m=\u001b[39m aubio_peakpicker_do(hfc, threshold\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py:87\u001b[0m, in \u001b[0;36mSpectrogram.__new__\u001b[0;34m(cls, stft, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py?line=83'>84</a>\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(stft)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py?line=84'>85</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py?line=85'>86</a>\u001b[0m     \u001b[39m# try to instantiate a ShortTimeFourierTransform\u001b[39;00m\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py?line=86'>87</a>\u001b[0m     stft \u001b[39m=\u001b[39m ShortTimeFourierTransform(stft, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py?line=87'>88</a>\u001b[0m     \u001b[39m# take the abs of the STFT\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/spectrogram.py?line=88'>89</a>\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(stft)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/madmom/audio/stft.py:359\u001b[0m, in \u001b[0;36mShortTimeFourierTransform.__new__\u001b[0;34m(cls, frames, window, fft_size, circular_shift, include_nyquist, fft_window, fftw, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=356'>357</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=357'>358</a>\u001b[0m \u001b[39m# calculate the STFT\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=358'>359</a>\u001b[0m data \u001b[39m=\u001b[39m stft(frames, fft_window, fft_size\u001b[39m=\u001b[39;49mfft_size,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=359'>360</a>\u001b[0m             circular_shift\u001b[39m=\u001b[39;49mcircular_shift,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=360'>361</a>\u001b[0m             include_nyquist\u001b[39m=\u001b[39;49minclude_nyquist, fftw\u001b[39m=\u001b[39;49mfftw)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=362'>363</a>\u001b[0m \u001b[39m# cast as ShortTimeFourierTransform\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=363'>364</a>\u001b[0m obj \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(data)\u001b[39m.\u001b[39mview(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/madmom/audio/stft.py:83\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(frames, window, fft_size, circular_shift, include_nyquist, fftw)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=79'>80</a>\u001b[0m \u001b[39m# check for correct shape of input\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=80'>81</a>\u001b[0m \u001b[39mif\u001b[39;00m frames\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=81'>82</a>\u001b[0m     \u001b[39m# TODO: add multi-channel support\u001b[39;00m\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=82'>83</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mframes must be a 2D array or iterable, got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=83'>84</a>\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(frames), frames\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=85'>86</a>\u001b[0m \u001b[39m# shape of the frames\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.9/site-packages/madmom/audio/stft.py?line=86'>87</a>\u001b[0m num_frames, frame_size \u001b[39m=\u001b[39m frames\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: frames must be a 2D array or iterable, got <class 'madmom.audio.signal.FramedSignal'> with shape (5663, 2048, 2)."
     ]
    }
   ],
   "source": [
    "CD_list = []\n",
    "FN_list = []\n",
    "FP_list = []\n",
    "for fold, (train_idx, test_idx) in enumerate(folds):\n",
    "    print(fold)\n",
    "    fold_average = []\n",
    "    for r in test_idx:\n",
    "        rec_name = os.path.basename(audio_fnames[r])\n",
    "        sig = madmom.audio.Signal(audio_fnames[r])\n",
    "        x = get_segmented_data(audio_fnames[r])\n",
    "        if neural:\n",
    "            model = tf.keras.models.load_model(base_path + \"fold_\" + str(fold) + \"_\" + model_name + \"_model\")\n",
    "            out = model.predict(x)\n",
    "            peaks = get_test_peaks(out, 1./FPS)\n",
    "        else:\n",
    "            spect = madmom.audio.spectrogram.Spectrogram(audio_fnames[r])\n",
    "            hfc = madmom.features.onsets.high_frequency_content(spect)\n",
    "            onehot, out = aubio_peakpicker_do(hfc, threshold=0.15)\n",
    "            peaks = aubio_postprocessing(onehot, sig, db_thres=-90, min_ioi_frames=6)/FPS\n",
    "            \n",
    "            #leg_on, leg_val = legato_mg(audio_fnames[r], rel_delta=0.2)\n",
    "            #peaks = onsets_threshold_gate(leg_on, leg_val, 1.0)\n",
    "        \n",
    "        [CD,FN,FP,doubles,merged] = evaluate(onset_schedules[r], peaks, tol_sec=TOL)\n",
    "        CD_list.append(CD)\n",
    "        FN_list.append(FN)\n",
    "        FP_list.append(FP)\n",
    "\n",
    "        scores = mir_eval.onset.evaluate(onset_schedules[r], peaks, window=TOL)\n",
    "        print(rec_name + \"\\t\" + \"F-score: {:.2f}\".format(100*scores[\"F-measure\"]))\n",
    "        fold_average.append(scores[\"F-measure\"])\n",
    "    print(\"Fold {}    average F-score {:.2f}\".format(fold, 100*np.mean(fold_average)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8505654281098546"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(CD_list)/(np.sum(CD_list)+.5*(np.sum(FP_list) + np.sum(FN_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
